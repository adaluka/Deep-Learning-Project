{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2BPwQrd15JXArQnlsdopq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adaluka/Deep-Learning-Project/blob/main/model/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE0SEUPGST2P",
        "outputId": "a2608ec6-f2b2-4c81-f37c-7e89607a0892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting funcy\n",
            "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
            "Installing collected packages: funcy\n",
            "Successfully installed funcy-1.17\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install funcy\n",
        "import funcy\n",
        "from funcy import print_durations\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler\n",
        "torch.backends.cudnn.benchmark = True\n",
        "import datetime\n",
        "import gc\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_date = '1980-01-01'"
      ],
      "metadata": {
        "id": "ncue1ppaTHeQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-0rVjyWTNqE",
        "outputId": "6b9874d3-d384-4000-cf47-86ec431b0bce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macro_df = pd.read_csv('/content/drive/MyDrive/11-785/Project/macro_features_final.csv',index_col=0).loc[start_date:]\n",
        "macro_df = macro_df.dropna(axis=0,how='all')\n",
        "macro_df = macro_df.dropna(axis=1,how='all')\n",
        "# features_idx = macro_df.columns\n",
        "macro_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "hj3cxdK_F8l1",
        "outputId": "da39769d-0c16-4dc2-8677-e76b51a94ff1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            W875RX1  DPCERA3M086SBEA     CMRMTSPL    INDPRO   IPFPNSS  \\\n",
              "rdq                                                                     \n",
              "1980-01-26   4985.4           39.122   596439.789   52.1712   55.8565   \n",
              "1980-01-31      NaN              NaN          NaN       NaN       NaN   \n",
              "1980-02-23   4965.6           38.795   590829.499   52.1976   56.0214   \n",
              "1980-02-29      NaN              NaN          NaN       NaN       NaN   \n",
              "1980-03-29   4948.3           38.479   577376.228   51.9834   55.6418   \n",
              "...             ...              ...          ...       ...       ...   \n",
              "2021-11-30  14406.3          125.640  1572476.000  102.0374  101.2518   \n",
              "2021-12-31  14398.0          123.868  1562935.000  101.6402  100.7942   \n",
              "2022-01-31  14363.1          126.504  1594543.000  102.6863  101.8134   \n",
              "2022-02-28  14373.4          126.039  1594543.000  103.6341  102.5059   \n",
              "2022-03-31  14373.4          126.039  1594543.000  104.5853  103.6178   \n",
              "\n",
              "             IPFINAL   IPCONGD  IPDCONGD  IPNCONGD  IPBUSEQ  ...       RNA  \\\n",
              "rdq                                                          ...             \n",
              "1980-01-26   54.1991   65.9046   44.8476   75.6224  30.9752  ...       NaN   \n",
              "1980-01-31       NaN       NaN       NaN       NaN      NaN  ...  0.121469   \n",
              "1980-02-23   54.5048   66.0877   44.5344   76.1270  31.1525  ...       NaN   \n",
              "1980-02-29       NaN       NaN       NaN       NaN      NaN  ...  0.121540   \n",
              "1980-03-29   54.1420   65.5822   43.7530   75.8414  30.8860  ...       NaN   \n",
              "...              ...       ...       ...       ...      ...  ...       ...   \n",
              "2021-11-30  101.6865  100.8504  104.7309   99.7321  95.8557  ...  0.049653   \n",
              "2021-12-31  101.2701  100.2659  103.9276   99.2067  95.7659  ...  0.049773   \n",
              "2022-01-31  102.4001  101.8697  106.0272  100.6766  95.4291  ...  0.049706   \n",
              "2022-02-28  102.7867  101.4960  103.9094  100.7686  97.1781  ...  0.049793   \n",
              "2022-03-31  104.3362  102.8988  107.9180  101.4761  98.8924  ...  0.092154   \n",
              "\n",
              "                 ROA       ROE     SGA2S       D2A        AC        OA  \\\n",
              "rdq                                                                      \n",
              "1980-01-26       NaN       NaN       NaN       NaN       NaN       NaN   \n",
              "1980-01-31  0.064200  0.154266  0.620194  0.033420  0.009393 -0.001824   \n",
              "1980-02-23       NaN       NaN       NaN       NaN       NaN       NaN   \n",
              "1980-02-29  0.063514  0.157833  0.623039  0.033588  0.011206 -0.001425   \n",
              "1980-03-29       NaN       NaN       NaN       NaN       NaN       NaN   \n",
              "...              ...       ...       ...       ...       ...       ...   \n",
              "2021-11-30  0.013872  0.069816  0.659218  0.022986  0.003672 -0.000196   \n",
              "2021-12-31  0.013892  0.069949  0.659782  0.022915  0.004113 -0.000202   \n",
              "2022-01-31  0.013879  0.070214  0.659905  0.022857  0.003672 -0.000198   \n",
              "2022-02-28  0.014213  0.071293  0.664258  0.022921  0.005843 -0.000217   \n",
              "2022-03-31  0.050156  0.139828  0.458543  0.025205  0.016476 -0.000192   \n",
              "\n",
              "                  OL       PCM         AT  \n",
              "rdq                                        \n",
              "1980-01-26       NaN       NaN        NaN  \n",
              "1980-01-31  1.265149 -1.788297   191.9680  \n",
              "1980-02-23       NaN       NaN        NaN  \n",
              "1980-02-29  1.276434 -1.806736   195.4105  \n",
              "1980-03-29       NaN       NaN        NaN  \n",
              "...              ...       ...        ...  \n",
              "2021-11-30  0.474068 -0.202465  1124.9105  \n",
              "2021-12-31  0.475670 -0.205471  1131.1950  \n",
              "2022-01-31  0.478045 -0.212693  1136.5500  \n",
              "2022-02-28  0.482213 -0.221224  1166.1680  \n",
              "2022-03-31  0.392938 -0.577736  4966.9855  \n",
              "\n",
              "[670 rows x 159 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a93bb9b-977b-4c88-9400-96a5578e8fc5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>W875RX1</th>\n",
              "      <th>DPCERA3M086SBEA</th>\n",
              "      <th>CMRMTSPL</th>\n",
              "      <th>INDPRO</th>\n",
              "      <th>IPFPNSS</th>\n",
              "      <th>IPFINAL</th>\n",
              "      <th>IPCONGD</th>\n",
              "      <th>IPDCONGD</th>\n",
              "      <th>IPNCONGD</th>\n",
              "      <th>IPBUSEQ</th>\n",
              "      <th>...</th>\n",
              "      <th>RNA</th>\n",
              "      <th>ROA</th>\n",
              "      <th>ROE</th>\n",
              "      <th>SGA2S</th>\n",
              "      <th>D2A</th>\n",
              "      <th>AC</th>\n",
              "      <th>OA</th>\n",
              "      <th>OL</th>\n",
              "      <th>PCM</th>\n",
              "      <th>AT</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rdq</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1980-01-26</th>\n",
              "      <td>4985.4</td>\n",
              "      <td>39.122</td>\n",
              "      <td>596439.789</td>\n",
              "      <td>52.1712</td>\n",
              "      <td>55.8565</td>\n",
              "      <td>54.1991</td>\n",
              "      <td>65.9046</td>\n",
              "      <td>44.8476</td>\n",
              "      <td>75.6224</td>\n",
              "      <td>30.9752</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-01-31</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121469</td>\n",
              "      <td>0.064200</td>\n",
              "      <td>0.154266</td>\n",
              "      <td>0.620194</td>\n",
              "      <td>0.033420</td>\n",
              "      <td>0.009393</td>\n",
              "      <td>-0.001824</td>\n",
              "      <td>1.265149</td>\n",
              "      <td>-1.788297</td>\n",
              "      <td>191.9680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-02-23</th>\n",
              "      <td>4965.6</td>\n",
              "      <td>38.795</td>\n",
              "      <td>590829.499</td>\n",
              "      <td>52.1976</td>\n",
              "      <td>56.0214</td>\n",
              "      <td>54.5048</td>\n",
              "      <td>66.0877</td>\n",
              "      <td>44.5344</td>\n",
              "      <td>76.1270</td>\n",
              "      <td>31.1525</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-02-29</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121540</td>\n",
              "      <td>0.063514</td>\n",
              "      <td>0.157833</td>\n",
              "      <td>0.623039</td>\n",
              "      <td>0.033588</td>\n",
              "      <td>0.011206</td>\n",
              "      <td>-0.001425</td>\n",
              "      <td>1.276434</td>\n",
              "      <td>-1.806736</td>\n",
              "      <td>195.4105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1980-03-29</th>\n",
              "      <td>4948.3</td>\n",
              "      <td>38.479</td>\n",
              "      <td>577376.228</td>\n",
              "      <td>51.9834</td>\n",
              "      <td>55.6418</td>\n",
              "      <td>54.1420</td>\n",
              "      <td>65.5822</td>\n",
              "      <td>43.7530</td>\n",
              "      <td>75.8414</td>\n",
              "      <td>30.8860</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-11-30</th>\n",
              "      <td>14406.3</td>\n",
              "      <td>125.640</td>\n",
              "      <td>1572476.000</td>\n",
              "      <td>102.0374</td>\n",
              "      <td>101.2518</td>\n",
              "      <td>101.6865</td>\n",
              "      <td>100.8504</td>\n",
              "      <td>104.7309</td>\n",
              "      <td>99.7321</td>\n",
              "      <td>95.8557</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049653</td>\n",
              "      <td>0.013872</td>\n",
              "      <td>0.069816</td>\n",
              "      <td>0.659218</td>\n",
              "      <td>0.022986</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>-0.000196</td>\n",
              "      <td>0.474068</td>\n",
              "      <td>-0.202465</td>\n",
              "      <td>1124.9105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2021-12-31</th>\n",
              "      <td>14398.0</td>\n",
              "      <td>123.868</td>\n",
              "      <td>1562935.000</td>\n",
              "      <td>101.6402</td>\n",
              "      <td>100.7942</td>\n",
              "      <td>101.2701</td>\n",
              "      <td>100.2659</td>\n",
              "      <td>103.9276</td>\n",
              "      <td>99.2067</td>\n",
              "      <td>95.7659</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049773</td>\n",
              "      <td>0.013892</td>\n",
              "      <td>0.069949</td>\n",
              "      <td>0.659782</td>\n",
              "      <td>0.022915</td>\n",
              "      <td>0.004113</td>\n",
              "      <td>-0.000202</td>\n",
              "      <td>0.475670</td>\n",
              "      <td>-0.205471</td>\n",
              "      <td>1131.1950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-01-31</th>\n",
              "      <td>14363.1</td>\n",
              "      <td>126.504</td>\n",
              "      <td>1594543.000</td>\n",
              "      <td>102.6863</td>\n",
              "      <td>101.8134</td>\n",
              "      <td>102.4001</td>\n",
              "      <td>101.8697</td>\n",
              "      <td>106.0272</td>\n",
              "      <td>100.6766</td>\n",
              "      <td>95.4291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049706</td>\n",
              "      <td>0.013879</td>\n",
              "      <td>0.070214</td>\n",
              "      <td>0.659905</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.003672</td>\n",
              "      <td>-0.000198</td>\n",
              "      <td>0.478045</td>\n",
              "      <td>-0.212693</td>\n",
              "      <td>1136.5500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-02-28</th>\n",
              "      <td>14373.4</td>\n",
              "      <td>126.039</td>\n",
              "      <td>1594543.000</td>\n",
              "      <td>103.6341</td>\n",
              "      <td>102.5059</td>\n",
              "      <td>102.7867</td>\n",
              "      <td>101.4960</td>\n",
              "      <td>103.9094</td>\n",
              "      <td>100.7686</td>\n",
              "      <td>97.1781</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049793</td>\n",
              "      <td>0.014213</td>\n",
              "      <td>0.071293</td>\n",
              "      <td>0.664258</td>\n",
              "      <td>0.022921</td>\n",
              "      <td>0.005843</td>\n",
              "      <td>-0.000217</td>\n",
              "      <td>0.482213</td>\n",
              "      <td>-0.221224</td>\n",
              "      <td>1166.1680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022-03-31</th>\n",
              "      <td>14373.4</td>\n",
              "      <td>126.039</td>\n",
              "      <td>1594543.000</td>\n",
              "      <td>104.5853</td>\n",
              "      <td>103.6178</td>\n",
              "      <td>104.3362</td>\n",
              "      <td>102.8988</td>\n",
              "      <td>107.9180</td>\n",
              "      <td>101.4761</td>\n",
              "      <td>98.8924</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092154</td>\n",
              "      <td>0.050156</td>\n",
              "      <td>0.139828</td>\n",
              "      <td>0.458543</td>\n",
              "      <td>0.025205</td>\n",
              "      <td>0.016476</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>0.392938</td>\n",
              "      <td>-0.577736</td>\n",
              "      <td>4966.9855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>670 rows Ã— 159 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a93bb9b-977b-4c88-9400-96a5578e8fc5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a93bb9b-977b-4c88-9400-96a5578e8fc5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a93bb9b-977b-4c88-9400-96a5578e8fc5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "returns_df = pd.read_csv('/content/drive/MyDrive/11-785/Project/returns.csv',index_col=0).loc[start_date:]\n",
        "returns_df = returns_df.dropna(axis=0,how='all')\n",
        "returns_df = returns_df.dropna(axis=1,how='all')\n",
        "returns_df.clip(returns_df.quantile(0.01,axis=1),returns_df.quantile(0.99,axis=1),axis=0)\n",
        "date_idx = returns_df.index\n",
        "equity_idx = returns_df.columns\n",
        "returns_df"
      ],
      "metadata": {
        "id": "adsQjvPJTJ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_comb_df = pd.read_csv('/content/drive/MyDrive/11-785/Project/features_full_final.csv').set_index(['tic','rdq']).drop(['FYEARQ','FQTR'],axis=1)\n",
        "feat_comb_df = feat_comb_df.dropna(axis=0,how='all')\n",
        "feat_comb_df = feat_comb_df.dropna(axis=1,how='all')\n",
        "feat_comb_df = feat_comb_df[sorted(feat_comb_df.columns)]\n",
        "feat_comb_df"
      ],
      "metadata": {
        "id": "oVBCJ7p8TmV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_AVAILABLE_FEATURES = feat_comb_df.shape[1]//2\n",
        "\n",
        "full_df_lst = []\n",
        "for date in tqdm(date_idx):\n",
        "    sub = feat_comb_df.loc[feat_comb_df.index.get_level_values(1) < date]\n",
        "    sub = sub.groupby(sub.index.get_level_values('tic')).last(1)\n",
        "    sub = sub.transpose()\n",
        "    \n",
        "    sub = sub.rank(pct=True,axis=1)*2-1\n",
        "    idx = (sub.isna().sum(axis=0) >= MIN_AVAILABLE_FEATURES).index\n",
        "    sub = sub[idx].fillna(0)\n",
        "    \n",
        "#     sub = sub.fillna(sub.median(axis=1),axis=0)\n",
        "    \n",
        "    sub['date'] = date\n",
        "    sub =  sub.set_index(['date',sub.index])\n",
        "    \n",
        "    full_df_lst.append(sub)"
      ],
      "metadata": {
        "id": "Md43XKu5UP8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del(feat_comb_df)\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "A7oclFBDaUr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig = (83,360)\n",
        "valid = (360,420)\n",
        "test = (420,505)"
      ],
      "metadata": {
        "id": "9W8E2oZmWu7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AEDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, macro_df, full_df_lst, returns_df, date_idx, start_i, end_i, rhs_input_assets=False,lag=1):\n",
        "        self.full_df_lst = full_df_lst\n",
        "        self.macro_df_idx = macro_df.index\n",
        "        self.macro_df = macro_df.to_numpy()\n",
        "        self.macro_df[np.isnan(self.macro_df)] = 0\n",
        "        self.returns_df = returns_df\n",
        "        self.date_idx = date_idx\n",
        "        self.start_i, self.end_i = start_i, end_i\n",
        "        self.rhs_input_assets = rhs_input_assets\n",
        "        self.epsilon=1e-5\n",
        "        self.lag=lag\n",
        "        \n",
        "        self.output_date_idx = date_idx[start_i+lag:end_i]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.end_i - self.start_i - self.lag\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        input_idx = idx\n",
        "        idx += self.start_i+self.lag\n",
        "        date = self.date_idx[idx] # returns date\n",
        "        \n",
        "        macro = self.macro_df\n",
        "        \n",
        "        mask = ~np.isnan(self.returns_df.loc[date].to_numpy())\n",
        "        traded_assets_idx = self.returns_df.columns[mask]\n",
        "        \n",
        "        lhs = self.full_df_lst[idx - self.lag]\n",
        "        lhs_idx = lhs.columns\n",
        "        \n",
        "        comb_idx = traded_assets_idx.intersection(lhs_idx)\n",
        "        \n",
        "        lhs = lhs.reindex(comb_idx,axis=1,fill_value=0).to_numpy()\n",
        "        \n",
        "        rhs_tmp = self.returns_df.loc[date,comb_idx]\n",
        "        rhs_tmp = rhs_tmp.clip(rhs_tmp.quantile(0.01),rhs_tmp.quantile(0.99))\n",
        "        output = rhs_tmp.to_numpy()\n",
        "\n",
        "        lhs_tmp = np.concatenate([lhs,np.expand_dims(np.ones(lhs.shape[1]),0)],axis=0)\n",
        "        \n",
        "        if self.rhs_input_assets:\n",
        "            rhs = output\n",
        "        else:\n",
        "            try:\n",
        "#               \n",
        "                # Causing singular error. Changing to pseudo inverse\n",
        "                rhs = (np.linalg.pinv(lhs_tmp @ lhs_tmp.T) @ lhs_tmp) @ rhs_tmp\n",
        "#               \n",
        "            except Exception as e:\n",
        "                print(\"idx:\",input_idx)\n",
        "                print(e)\n",
        "#                 return self.full_df_lst[idx - self.lag].reindex(traded_assets_idx,axis=1,fill_value=0), rhs_tmp\n",
        "                # return lhs,lhs_tmp,output\n",
        "                raise e\n",
        "#             rhs.index = lhs_tmp.index\n",
        "#             rhs = rhs.reindex(self.alpha_names,fill_value=0).clip(-5e4,5e4)\n",
        "#             rhs['equal'] = equal_weighted_portfolio\n",
        "#             rhs = rhs.to_numpy()\n",
        "        return macro,self.macro_df_idx,date,lhs,rhs,output,comb_idx\n",
        "        \n",
        "        \n",
        "def collate_fn(batch):\n",
        "    macro = batch[0][0]\n",
        "    macro_df_idx = batch[0][1]\n",
        "    dates = [i for _,_,i,_,_,_,_ in batch]\n",
        "    len_x = [len(output) for _,_,_,_,_,output,_ in batch]\n",
        "    full_size = max(len_x)\n",
        "#     print(full_size)\n",
        "    \n",
        "    \n",
        "    batch_lhs = [nn.functional.pad(torch.Tensor(lhs),pad=(0,full_size-lhs.shape[1],0,0)) for macro,_,_,lhs,rhs,output,_ in batch]\n",
        "    batch_rhs = [torch.Tensor(rhs) for macro,_,_,lhs,rhs,output,_ in batch]\n",
        "    batch_output = [nn.functional.pad(torch.Tensor(output),pad=(0,full_size-len(output))) for macro,_,_,lhs,rhs,output,_ in batch]\n",
        "    batch_assets = [idx for macro,_,_,lhs,rhs,output,idx in batch]\n",
        "\n",
        "    \n",
        "    return torch.from_numpy(macro), torch.from_numpy(np.isin(macro_df_idx.to_numpy(), dates)),\\\n",
        "        torch.cat([x.unsqueeze(0) for x in batch_lhs]),\\\n",
        "        torch.cat([x.unsqueeze(0) for x in batch_rhs]),\\\n",
        "        torch.cat([x.unsqueeze(0) for x in batch_output]), torch.tensor(len_x), batch_assets"
      ],
      "metadata": {
        "id": "EVUqm-DEWx68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildListFromConfigs(config, inp_size, out_size, batch_norm=True, dropout=0.5, initialization=False, nonlinear=nn.SELU):\n",
        "    layer_lst = []\n",
        "    for i, v in enumerate(list(config)+[out_size]):\n",
        "        if batch_norm and i > 0: #generally no batch norm before the model starts\n",
        "            layer_lst.append(nn.LayerNorm(inp_size))\n",
        "        if dropout > 1e-8 and i > 0:\n",
        "            layer_lst.append(nn.Dropout(dropout))\n",
        "        layer_lst.append(nn.Linear(inp_size, v, bias=True))\n",
        "        if initialization:\n",
        "            nn.init.kaiming_normal_(layer_lst[-1].weight,mode='fan_in',nonlinearity='linear')\n",
        "        if i < len(config):\n",
        "            layer_lst.append(nonlinear())\n",
        "        inp_size = v\n",
        "    return layer_lst, inp_size\n",
        "\n",
        "class BuildLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout, proj_size=None):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = nn.ModuleList([nn.Dropout(dropout) for i in range(num_layers)]) #nn.ModuleList([LockedDropout(dropout) for i in range(num_layers)])\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(nn.LSTMCell(input_size, hidden_size))\n",
        "            input_size = hidden_size\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.final = nn.Linear(input_size, proj_size) if proj_size is not None and input_size != proj_size else nn.Identity()\n",
        "    \n",
        "    def forward(self, data, hidden=None):\n",
        "        if hidden is None:\n",
        "            hidden = [None for i in range(self.num_layers)]\n",
        "        for i in range(self.num_layers):\n",
        "            data = self.dropout[i](data)\n",
        "            data, hidden[i] = self.layers[i](data, hidden[i])\n",
        "        data = self.final(data)\n",
        "        return data, hidden\n",
        "        \n",
        "\n",
        "class ConditionalAutoEncoder(nn.Module):\n",
        "    # starting macro is the state of the last 13 months. For training, set it to None (equivalent to setting it to a torch of zeros).\n",
        "    # However, since testing data is low, run the macro net on the past <attention_span> months and send that. First month's data is not used.\n",
        "    # Requires a <attention_span>*<n_macro_size> torch vector in the same device as the model, where n_macro_size is the output of the n_hidden_macro net.\n",
        "    def __init__(self, n_alphas, n_factors, n_macro, n_hidden_macro, n_hidden_lhs, n_hidden_rhs, attention_vs_lstm='attention', starting_macro = None, starting_rhs = None,\n",
        "                    batch_norm_lhs=True, batch_norm_rhs=False, dropout_p=0, initialization=True, rhs_input_assets = False, rhs_activation = True,\n",
        "                    bn_momentum=0.1, bn_track_running_stats = True, macro_attention_span=1, rhs_attention_span=1, num_lstm_layers=1):\n",
        "        # n_assets can be assets or portfolios:\n",
        "            # In both cases, we are interested in asset return or portfolio return on the RHS\n",
        "            # LHS is the original factor \n",
        "        super(ConditionalAutoEncoder, self).__init__()\n",
        "\n",
        "        self.n_alphas = n_alphas\n",
        "        self.n_factors = n_factors\n",
        "        self.n_hidden_lhs = list(n_hidden_lhs)\n",
        "        self.n_hidden_rhs = list(n_hidden_rhs)\n",
        "        self.n_macro = n_macro\n",
        "        self.n_hidden_macro = n_hidden_macro\n",
        "        self.lstm = attention_vs_lstm != 'attention'\n",
        "\n",
        "        if attention_vs_lstm == 'attention':\n",
        "            self.prev_macro = torch.zeros((macro_attention_span, n_factors), requires_grad=True) if starting_macro is None else starting_macro\n",
        "            self.prev_rhs   = torch.zeros((rhs_attention_span, n_factors), requires_grad=True) if starting_rhs is None else starting_rhs\n",
        "            self.macro_attention_span = macro_attention_span\n",
        "            self.rhs_attention_span = rhs_attention_span\n",
        "            #attention_lst = buildListFromConfigs(n_hidden_macro, n_macro+macro_attention_span*n_factors + rhs_attention_span*n_factors, n_factors, batch_norm=True, dropout=dropout_p, initialization=True, nonlinear=nn.SELU)\n",
        "            attention_lst, _ = buildListFromConfigs(n_hidden_macro, n_macro+macro_attention_span*n_factors + rhs_attention_span*n_factors, n_factors, batch_norm=True, dropout=dropout_p, initialization=True, nonlinear=nn.SELU)\n",
        "            self.macro = nn.Sequential(*attention_lst)\n",
        "        else:\n",
        "            attention_lst, _ = buildListFromConfigs(n_hidden_macro, n_macro, n_factors, batch_norm=True, dropout=dropout_p, initialization=True, nonlinear=nn.SELU)\n",
        "            self.macro = nn.Sequential(*attention_lst)\n",
        "            self.macro_lstm = nn.LSTM(n_factors, n_factors, num_lstm_layers, dropout=dropout_p) #BuildLSTM(n_factors, n_factors, num_lstm_layers)\n",
        "\n",
        "        \n",
        "        lhs_lst, _ = buildListFromConfigs(n_hidden_lhs, n_alphas, n_factors, batch_norm_lhs, dropout=dropout_p, initialization=True, nonlinear=nn.SELU)\n",
        "        rhs_lst, _ = buildListFromConfigs(n_hidden_rhs, n_alphas+1, n_factors, batch_norm_rhs, dropout=dropout_p, initialization=True, nonlinear=nn.SELU if rhs_activation else nn.Identity)\n",
        "        if rhs_activation:\n",
        "            rhs_lst.append(nn.SELU()) #Still not sold on this\n",
        "\n",
        "        \n",
        "        self.lhs = nn.Sequential(*lhs_lst)\n",
        "        self.rhs = nn.Sequential(*rhs_lst)\n",
        "    \n",
        "    def run_through_lhs(self, data):\n",
        "        #y_lhs_lst = []\n",
        "        #for i in range(x_lhs.shape[2]):\n",
        "        #    y_cur = self.lhs(x_lhs[:,:,i])\n",
        "        #    y_cur = torch.unsqueeze(y_cur,2)\n",
        "        #    y_lhs_lst.append(y_cur)\n",
        "        #y_lhs = torch.cat(y_lhs_lst,dim=2)\n",
        "\n",
        "        shape = data.shape\n",
        "        data = data.permute(0, 2, 1).reshape(-1, shape[1])\n",
        "        out = self.lhs(data)\n",
        "        return out.reshape(shape[0], shape[2], -1).permute(0, 2, 1)\n",
        "    \n",
        "    def run_through_macro(self, x_macro, y_rhs, idx):\n",
        "        if self.lstm:\n",
        "            y_macro = self.macro(x_macro[:, :])\n",
        "            y_macro, hidden = self.macro_lstm(y_macro[None, :, :])\n",
        "            y_macro = y_macro[0, idx, :][:, :, None]\n",
        "        else:\n",
        "            # If using Attention, store prev_macro and prev_rhs outside like lstm\n",
        "            macro = torch.cat((x_macro, self.prev_macro.reshape(1, -1), self.prev_rhs.reshape(1, -1)))\n",
        "            y_macro = self.macro(macro)\n",
        "            hidden = None\n",
        "            self.prev_macro = torch.roll(self.prev_macro, -1, 0)\n",
        "            self.prev_macro[-1] = y_macro\n",
        "            self.prev_rhs = torch.roll(self.prev_rhs, -1, 0)\n",
        "            self.prev_rhs[-1] = y_rhs\n",
        "        return y_macro, hidden\n",
        "\n",
        "    def forward(self, x_macro, macro_idx, x_lhs, x_rhs):\n",
        "        y_lhs = self.run_through_lhs(x_lhs)\n",
        "        y_rhs = self.rhs(x_rhs).unsqueeze(dim=2)\n",
        "        self.last_rhs_factors = y_rhs\n",
        "        y_macro, hidden = self.run_through_macro(x_macro, y_rhs, macro_idx)\n",
        "        out = y_lhs*y_rhs*y_macro\n",
        "        out = torch.sum(out, dim=1) #Double check this\n",
        "        return out, y_rhs\n",
        "\n",
        "    def forward_given_factors(self, x_macro, macro_idx, x_lhs, factors):\n",
        "        y_lhs = self.run_through_lhs(x_lhs)\n",
        "        y_rhs = factors\n",
        "        y_macro, hidden = self.run_through_macro(x_macro, y_rhs, macro_idx)\n",
        "        #out = torch.bmm(y_lhs.transpose(1,2), y_rhs).squeeze(2)\n",
        "        out = torch.sum(y_lhs*y_rhs*y_macro, dim=1) #Double check this\n",
        "        return out"
      ],
      "metadata": {
        "id": "XHrSoJdbW1Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "@print_durations()\n",
        "def test(args,model,device,test_loader,criterion):\n",
        "    model.eval()\n",
        "        \n",
        "    output_lst = []\n",
        "    data_lst = []\n",
        "\n",
        "    for data_macro, idx_macro, data_lhs, data_rhs, data_output, data_x_lens, data_assets in test_loader:\n",
        "        # print(\"Shapes:\",data_lhs.shape, data_rhs.shape, data_output.shape, data_mask.shape)\n",
        "        idx_macro = idx_macro.to(device)\n",
        "        data_macro = data_macro.float().to(device)\n",
        "        data_lhs = data_lhs.float().to(device)\n",
        "        data_rhs = data_rhs.float().to(device)\n",
        "        data_output = data_output.float().to(device)\n",
        "        data_x_lens = data_x_lens.int().to(device)\n",
        "        output,_ = model(x_macro = data_macro, macro_idx=idx_macro, x_lhs = data_lhs, x_rhs = data_rhs)\n",
        "\n",
        "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
        "        output_masked = torch.masked_select(output,data_mask)\n",
        "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
        "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
        "        # print(\"Loss Inside:\",criterion(output_masked,data_output_masked))\n",
        "\n",
        "        output_lst.append(output_masked)\n",
        "        data_lst.append(data_output_masked)\n",
        "    \n",
        "    full_output = torch.cat(output_lst,axis=0)\n",
        "    full_data = torch.cat(data_lst,axis=0)\n",
        "\n",
        "    return mean_squared_error(full_data.cpu().detach().numpy(), full_output.cpu().detach().numpy())\n",
        "\n",
        "@print_durations()\n",
        "def train(args, model, device, train_loader, optimizer, criterion, epoch, max_norm=None):\n",
        "    model.train()\n",
        "    \n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "    \n",
        "    total_loss = 0\n",
        "    total_reg_loss=0\n",
        "    \n",
        "    N = len(train_loader)\n",
        "    \n",
        "    for batch_idx, (data_macro, idx_macro, data_lhs, data_rhs, data_output, data_x_lens, data_assets) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        idx_macro = idx_macro.to(device)\n",
        "        data_macro = data_macro.float().to(device)\n",
        "        data_lhs = data_lhs.float().to(device)\n",
        "        data_rhs = data_rhs.float().to(device)\n",
        "        data_output = data_output.float().to(device)\n",
        "        data_x_lens = data_x_lens.int().to(device)\n",
        "\n",
        "        if max_norm: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "        output,_ = model(x_macro = data_macro, macro_idx=idx_macro, x_lhs = data_lhs, x_rhs = data_rhs)\n",
        "        \n",
        "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
        "\n",
        "        output_masked = torch.masked_select(output,data_mask)\n",
        "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
        "\n",
        "        loss_tmp = criterion(output_masked, data_output_masked)\n",
        "\n",
        "        regularization_loss = 0\n",
        "        for p in model.parameters():\n",
        "            regularization_loss += torch.sum(abs(p))\n",
        "\n",
        "        loss = loss_tmp + args['l1_lambda'] * regularization_loss\n",
        "        \n",
        "        loss.backward()\n",
        "        total_loss += loss\n",
        "        total_reg_loss += regularization_loss * args['l1_lambda'] \n",
        "\n",
        "        # print(\"Gradient:\", loss.grad)\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            total_loss=\"{:.04f}\".format(float(total_loss / (batch_idx + 1))),\n",
        "            reg_loss = \"{:.04f}\".format(float(total_reg_loss / (batch_idx + 1))),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        batch_bar.update()\n",
        "        \n",
        "    print(f\"Epoch {epoch}, total_loss={float(total_loss / N):.04f}, reg_loss = {float(total_reg_loss / N):.04f}\")"
      ],
      "metadata": {
        "id": "X_9qSyM-W7lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(args, n_alphas, full_df_lst, returns_df, date_idx, train_start,\n",
        "        train_end, valid_start, valid_end, n_macro, n_window=36, batch_size=16, checkpoint_name=None):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = ConditionalAutoEncoder(\n",
        "        n_alphas = n_alphas,\n",
        "        n_factors = args['n_factors'],\n",
        "        n_macro = n_macro,\n",
        "        n_hidden_macro = args['n_hidden_macro'],\n",
        "        n_hidden_lhs = args['n_hidden_lhs'],\n",
        "        n_hidden_rhs = args['n_hidden_rhs'],\n",
        "        attention_vs_lstm='lstm',\n",
        "        batch_norm_lhs = args['batch_norm_lhs'],\n",
        "        batch_norm_rhs = args['batch_norm_rhs'],\n",
        "        dropout_p=args['dropout_p'],\n",
        "        initialization=args['initialization'],\n",
        "        rhs_input_assets=args['rhs_input_assets'],\n",
        "        rhs_activation = args['rhs_activation'],\n",
        "        num_lstm_layers = args['num_lstm_layers']\n",
        "    #     bn_track_running_stats=args['bn_track_running_stats'],\n",
        "    #     bn_momentum=args['bn_momentum']\n",
        "    )\n",
        "    \n",
        "    print(model)\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    train_data = AEDataSet(macro_df,full_df_lst,returns_df,date_idx,train_start,train_end)\n",
        "    valid_data = AEDataSet(macro_df,full_df_lst,returns_df,date_idx,valid_start,valid_end)\n",
        "    extended_valid_data= AEDataSet(macro_df,full_df_lst,returns_df,date_idx,valid_start-n_window,valid_end)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data,\n",
        "        batch_size = batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=0,\n",
        "        shuffle=True,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_data,\n",
        "        batch_size = batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=0,\n",
        "        shuffle=False,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "    \n",
        "    extended_valid_loader = torch.utils.data.DataLoader(\n",
        "        extended_valid_data,\n",
        "        batch_size = batch_size,\n",
        "        pin_memory=True,\n",
        "        num_workers=0,\n",
        "        shuffle=False,\n",
        "        collate_fn = collate_fn\n",
        "    )\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['l2_lambda'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    train_mse = test(args,model,device,train_loader,criterion)\n",
        "    valid_mse = test(args,model,device,valid_loader,criterion)\n",
        "    train_mse_lst, valid_mse_lst = [train_mse], [valid_mse]\n",
        "\n",
        "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['gamma'])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',0.5,patience=5,threshold=1e-3,cooldown=10)\n",
        "    print(\"Pre-Train Train MSE:\", train_mse)\n",
        "    print(\"Pre-Train Valid MSE:\", valid_mse)\n",
        "\n",
        "    best_model = None\n",
        "    for epoch in range(1, args['epoch']+1):\n",
        "        log_dict = {}\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        train(args,model,device,train_loader,optimizer,criterion,epoch,max_norm=args['max_norm'])\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        log_dict['train_mse'] = test(args,model,device,train_loader,criterion)\n",
        "        log_dict['valid_mse'] = test(args,model,device,valid_loader,criterion)\n",
        "        log_dict['lr'] = scheduler.optimizer.state_dict()['param_groups'][0]['lr']\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        print(\"Train MSE:\",log_dict['train_mse'])\n",
        "        print(\"Valid MSE:\",log_dict['valid_mse'])\n",
        "\n",
        "        scheduler.step(log_dict['valid_mse'])\n",
        "        print('Next Learning Rate:', scheduler.optimizer.state_dict()['param_groups'][0]['lr'])\n",
        "\n",
        "        print(train_mse_lst)\n",
        "        print(valid_mse_lst)\n",
        "        if log_dict['valid_mse'] < np.min(valid_mse_lst):\n",
        "            best_model = model\n",
        "            torch.save(best_model, './best_model.model')\n",
        "            # torch.save(model.state_dict(), args['MODEL_PATH']+\"/model_epoch_\"+str(epoch))\n",
        "\n",
        "        train_mse_lst.append(log_dict['train_mse'])\n",
        "        valid_mse_lst.append(log_dict['valid_mse'])\n",
        "\n",
        "        if len(valid_mse_lst)>args['patience'] and log_dict['valid_mse'] > \\\n",
        "            np.min(valid_mse_lst[:-args['patience']]):\n",
        "                break\n",
        "    \n",
        "    if checkpoint_name:\n",
        "        torch.save({\n",
        "#           'epoch': epoch,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "#           'scheduler_state_dict': scheduler.state_dict()\n",
        "          }, f'{checkpoint_name}.checkpoint')\n",
        "    \n",
        "    print(\"Training done. Evaluating...\")\n",
        "    \n",
        "    output_lst = []\n",
        "    data_lst = []\n",
        "    factors_lst = []\n",
        "\n",
        "    for data_macro, idx_macro, data_lhs, data_rhs, data_output, data_x_lens, data_assets in tqdm(extended_valid_loader):\n",
        "        # print(\"Shapes:\",data_lhs.shape, data_rhs.shape, data_output.shape, data_mask.shape)\n",
        "        idx_macro = idx_macro.to(device)\n",
        "        data_macro = data_macro.float().to(device)\n",
        "        data_lhs = data_lhs.float().to(device)\n",
        "        data_rhs = data_rhs.float().to(device)\n",
        "        data_output = data_output.float().to(device)\n",
        "        data_x_lens = data_x_lens.int().to(device)\n",
        "        output, factors = best_model(x_macro = data_macro, macro_idx=idx_macro, x_lhs = data_lhs, x_rhs = data_rhs)\n",
        "\n",
        "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
        "        output_masked = torch.masked_select(output,data_mask)\n",
        "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
        "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
        "\n",
        "        indices = [0]+list(torch.cumsum(data_x_lens,axis=0).cpu().numpy())\n",
        "\n",
        "        for k,(i,j) in enumerate(zip(indices[:-1],indices[1:])):\n",
        "            output_lst.append(pd.Series(output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
        "            data_lst.append(pd.Series(data_output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
        "            factors_lst.append(pd.Series(factors[k].squeeze().cpu().detach().numpy()))\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    output_df = pd.DataFrame(output_lst,index=extended_valid_data.output_date_idx).iloc[n_window:]\n",
        "    data_output_df = pd.DataFrame(data_lst,index=extended_valid_data.output_date_idx).iloc[n_window:]\n",
        "    factors_df = pd.DataFrame(factors_lst,index=extended_valid_data.output_date_idx)\n",
        "    \n",
        "    predicted_factors = factors_df.rolling(n_window).mean()\n",
        "    factors_tensor = torch.tensor(predicted_factors.iloc[n_window:].to_numpy()).unsqueeze(2).float().to(device)\n",
        "    \n",
        "    \n",
        "    pred_output_lst = []\n",
        "    for i, (data_macro, idx_macro, data_lhs, _, _, data_x_lens, data_assets) in tqdm(enumerate(valid_loader),total=len(valid_loader)):\n",
        "        idx_macro = idx_macro.to(device)\n",
        "        data_macro = data_macro.float().to(device)\n",
        "        data_lhs = data_lhs.float().to(device)\n",
        "        data_x_lens = data_x_lens.int().to(device)\n",
        "        output= best_model.forward_given_factors(data_macro, macro_idx=idx_macro, x_lhs = data_lhs, factors = factors_tensor[i*batch_size:(i+1)*batch_size])\n",
        "\n",
        "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
        "        output_masked = torch.masked_select(output,data_mask)\n",
        "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
        "\n",
        "        indices = [0]+list(torch.cumsum(data_x_lens,axis=0).cpu().numpy())\n",
        "\n",
        "        for k,(i,j) in enumerate(zip(indices[:-1],indices[1:])):\n",
        "            pred_output_lst.append(pd.Series(output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
        "    tm1_predicted_returns = pd.DataFrame(pred_output_lst,index=predicted_factors.iloc[n_window:].index)\n",
        "    predicted_returns = tm1_predicted_returns.shift(1).iloc[1:]\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    log_dict = {}\n",
        "    \n",
        "#     log_dict['predicted_returns'] = predicted_returns\n",
        "    \n",
        "    # Predictive Stats:\n",
        "    # Spearman Corr\n",
        "    corr = predicted_returns.corrwith(returns_df.loc[predicted_returns.index,predicted_returns.columns],axis=1,method='spearman')\n",
        "    log_dict['pred_spearman_corr'] = corr.mean()\n",
        "    # R2\n",
        "    numer = (returns_df.loc[predicted_returns.index,predicted_returns.columns]-predicted_returns)\n",
        "    numer_mask = (~numer.isna()).astype(int)\n",
        "    denom = returns_df.loc[predicted_returns.index,predicted_returns.columns] * numer_mask\n",
        "    log_dict['pred_r2'] = (1 - ((numer**2).sum(axis=1))/(denom**2).sum(axis=1)).mean()\n",
        "    \n",
        "    # Attribution Stats:\n",
        "    # Spearman Corr\n",
        "    corr = output_df.corrwith(returns_df.loc[output_df.index,output_df.columns],axis=1,method='spearman')\n",
        "    log_dict['total_spearman_corr'] = corr.mean()\n",
        "    # R2\n",
        "    numer = (returns_df.loc[output_df.index,output_df.columns]-output_df)\n",
        "    numer_mask = (~numer.isna()).astype(int)\n",
        "    denom = returns_df.loc[output_df.index,output_df.columns] * numer_mask\n",
        "    log_dict['total_r2'] = (1 - ((numer**2).sum(axis=1))/(denom**2).sum(axis=1)).mean()\n",
        "    \n",
        "    # Trading Stats:\n",
        "    train_pred_rank = predicted_returns.rank(pct=True,axis=1,numeric_only=True,ascending=True)\n",
        "    long_port = ((train_pred_rank >= 0.9) & (train_pred_rank < 1)).astype(int)\n",
        "    long_port = long_port.divide(long_port.sum(axis=1),axis=0)\n",
        "    long_port_returns = (long_port*returns_df.loc[long_port.index]).sum(axis=1)\n",
        "    short_port = (train_pred_rank <= 0.1).astype(int)\n",
        "    short_port = short_port.divide(short_port.sum(axis=1),axis=0)\n",
        "    short_port_returns = (short_port*returns_df.loc[short_port.index]).sum(axis=1)\n",
        "    comb_port = long_port - short_port\n",
        "    comb_port_returns = (comb_port*returns_df.loc[comb_port.index]).sum(axis=1)\n",
        "    log_dict['long_short_SR'] = (comb_port_returns.mean()/comb_port_returns.std())*np.sqrt(12)\n",
        "    log_dict['long_short_returns'] = comb_port_returns.mean()*12\n",
        "    log_dict['long_short_SD'] = comb_port_returns.std()*np.sqrt(12)\n",
        "    \n",
        "    log_dict['long_SR'] = (long_port_returns.mean()/long_port_returns.std())*np.sqrt(12)\n",
        "    log_dict['long_returns'] = long_port_returns.mean()*12\n",
        "    log_dict['long_SD'] = long_port_returns.std()*np.sqrt(12)\n",
        "    \n",
        "    log_dict['short_SR'] = (short_port_returns.mean()/short_port_returns.std())*np.sqrt(12)\n",
        "    log_dict['short_returns'] = short_port_returns.mean()*12\n",
        "    log_dict['short_SD'] = short_port_returns.std()*np.sqrt(12)\n",
        "    \n",
        "    return log_dict"
      ],
      "metadata": {
        "id": "g3g40062XCzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_args = {\n",
        "#     'n_factors' : 5,\n",
        "#     'n_hidden_lhs':[16,8],\n",
        "    'n_hidden_rhs':[],\n",
        "    'batch_norm_lhs' : True,\n",
        "    'batch_norm_rhs' : False,\n",
        "#     'rhs_activation': True,\n",
        "    'dropout_p' : 0.0,\n",
        "    'max_norm' : 1,\n",
        "    'bn_momentum': 0.01,\n",
        "    'bn_track_running_stats': True,\n",
        "    'initialization' : False,\n",
        "    'rhs_input_assets' : False,\n",
        "    'lr' : 5e-4,\n",
        "    'gamma': 1/4,\n",
        "    'l2_lambda': 0,\n",
        "    'l1_lambda': 4e-3,\n",
        "    'batch_size': 32,\n",
        "    'log_interval': 1,\n",
        "    'epoch': 60,\n",
        "    'patience': 6\n",
        "}\n",
        "\n",
        "search_args = {\n",
        "    'n_factors': [4],\n",
        "#     'n_factors': [4],\n",
        "    'n_hidden_lhs': [(16,8)],\n",
        "#     'n_hidden_lhs': [(16,8)],\n",
        "    'rhs_activation': [False],\n",
        "    'n_hidden_macro': [(16, 8), (16)],\n",
        "    'num_lstm_layers': [2, 4, 8]\n",
        "}\n",
        "\n",
        "# fixed_args = {\n",
        "# #     'n_factors' : 5,\n",
        "# #     'n_hidden_lhs':[16,8],\n",
        "#     'n_hidden_rhs':[],\n",
        "#     'batch_norm_lhs' : True,\n",
        "#     'batch_norm_rhs' : False,\n",
        "#     'rhs_activation': True,\n",
        "#     'dropout_p' : 0.0,\n",
        "#     'max_norm' : 1,\n",
        "#     'bn_momentum': 0.01,\n",
        "#     'bn_track_running_stats': True,\n",
        "#     'initialization' : False,\n",
        "# #     'rhs_input_assets' : False,\n",
        "#     'lr' : 5e-4,\n",
        "#     'gamma': 1/4,\n",
        "#     'l2_lambda': 0,\n",
        "#     'l1_lambda': 4e-3,\n",
        "#     'batch_size': 32,\n",
        "#     'log_interval': 1,\n",
        "#     'epoch': 3,\n",
        "#     'patience': 6\n",
        "# }\n",
        "\n",
        "# search_args = {\n",
        "#     'n_factors': [2],\n",
        "#     'n_hidden_lhs': [(8,)],\n",
        "#     'rhs_input_assets': [True]\n",
        "# }\n",
        "\n",
        "search_keys = list(search_args.keys())\n",
        "search_values = list(search_args.values())\n",
        "from itertools import product\n",
        "search_values = list(product(*search_values))"
      ],
      "metadata": {
        "id": "4_HjXLxVXF2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_dict = {}\n",
        "\n",
        "if os.path.isfile('/content/drive/MyDrive/11-785/Project/res_dict_midterm_corrected.pickle'):\n",
        "    with open('/content/drive/MyDrive/11-785/Project/res_dict_midterm_corrected.pickle', 'rb') as handle:\n",
        "        res_dict = pickle.load(handle)\n",
        "\n",
        "for i,tpl in enumerate(search_values):\n",
        "    print(f\"Params {i}:\", tpl)\n",
        "    if i < len(res_dict):\n",
        "        continue\n",
        "    args = fixed_args.copy()\n",
        "    for k,v in zip(search_keys,tpl):\n",
        "        args[k] = v\n",
        "        \n",
        "    str_tpl = [str(tpl[0]),\"_\".join(list(map(str,tpl[1]))),\"activation\" if tpl[2] else \"noactivation\"]\n",
        "\n",
        "    log_dict = run(args,n_alphas=45, full_df_lst=full_df_lst, returns_df=returns_df, date_idx=date_idx, \n",
        "                   train_start=48, train_end=360, valid_start=360, valid_end=420, \n",
        "                   n_window=36, batch_size=16,\n",
        "                    checkpoint_name=f'midterm_corrected_layers{str_tpl[0]}_hidden_{str_tpl[1]}_{str_tpl[2]}',\n",
        "                   n_macro=159\n",
        "#                    checkpoint_name=f'midterm_corrected/layers{str_tpl[0]}_hidden_{str_tpl[1]}_{str_tpl[2]}'\n",
        "                  )\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    res_dict[tpl] = log_dict\n",
        "    \n",
        "    with open('/content/drive/MyDrive/11-785/Project/res_dict_midterm_corrected.pickle', 'wb') as handle:\n",
        "        pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    !cp * ../efs/project"
      ],
      "metadata": {
        "id": "sKjtB9qqKAt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macro_df"
      ],
      "metadata": {
        "id": "Hkm86aHEkR1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}