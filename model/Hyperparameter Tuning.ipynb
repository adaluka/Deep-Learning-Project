{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92d6558f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip install funcy\n",
    "import funcy\n",
    "from funcy import print_durations\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import datetime\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a150393",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1980-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae548f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0081A</th>\n",
       "      <th>0091A</th>\n",
       "      <th>0099A</th>\n",
       "      <th>0127A</th>\n",
       "      <th>0138A</th>\n",
       "      <th>0146A</th>\n",
       "      <th>0153A</th>\n",
       "      <th>0183B</th>\n",
       "      <th>0223B</th>\n",
       "      <th>0230B</th>\n",
       "      <th>...</th>\n",
       "      <th>ZVXI</th>\n",
       "      <th>ZWRK</th>\n",
       "      <th>ZWS</th>\n",
       "      <th>ZXAIY</th>\n",
       "      <th>ZY</th>\n",
       "      <th>ZYME</th>\n",
       "      <th>ZYNE</th>\n",
       "      <th>ZYXI</th>\n",
       "      <th>ZZ</th>\n",
       "      <th>ZZ1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datadate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.012579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-02-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.164835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-03-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.306250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.118421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-04-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.252252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.126761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.029851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-05-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.177419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036336</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.123077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>-0.033076</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>-0.145283</td>\n",
       "      <td>-0.096180</td>\n",
       "      <td>-0.096606</td>\n",
       "      <td>0.021548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>0.038516</td>\n",
       "      <td>0.054384</td>\n",
       "      <td>-0.261589</td>\n",
       "      <td>-0.184983</td>\n",
       "      <td>-0.167630</td>\n",
       "      <td>-0.221094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.004620</td>\n",
       "      <td>-0.160989</td>\n",
       "      <td>0.084211</td>\n",
       "      <td>-0.222720</td>\n",
       "      <td>-0.456986</td>\n",
       "      <td>-0.093750</td>\n",
       "      <td>-0.115145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.065815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.263462</td>\n",
       "      <td>-0.186517</td>\n",
       "      <td>-0.134100</td>\n",
       "      <td>-0.205808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 32354 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0081A     0091A  0099A     0127A     0138A  0146A  0153A  0183B  \\\n",
       "datadate                                                                      \n",
       "1980-01-31    NaN  0.012579    NaN  0.068182  0.039548    NaN    NaN    NaN   \n",
       "1980-02-29    NaN -0.006211    NaN  0.000000       NaN    NaN    NaN    NaN   \n",
       "1980-03-31    NaN -0.306250    NaN  0.007092       NaN    NaN    NaN    NaN   \n",
       "1980-04-30    NaN  0.252252    NaN -0.126761       NaN    NaN    NaN    NaN   \n",
       "1980-05-31    NaN  0.071942    NaN  0.177419       NaN    NaN    NaN    NaN   \n",
       "...           ...       ...    ...       ...       ...    ...    ...    ...   \n",
       "2021-11-30    NaN       NaN    NaN       NaN       NaN    NaN    NaN    NaN   \n",
       "2021-12-31    NaN       NaN    NaN       NaN       NaN    NaN    NaN    NaN   \n",
       "2022-01-31    NaN       NaN    NaN       NaN       NaN    NaN    NaN    NaN   \n",
       "2022-02-28    NaN       NaN    NaN       NaN       NaN    NaN    NaN    NaN   \n",
       "2022-03-31    NaN       NaN    NaN       NaN       NaN    NaN    NaN    NaN   \n",
       "\n",
       "               0223B  0230B  ...  ZVXI      ZWRK       ZWS     ZXAIY  \\\n",
       "datadate                     ...                                       \n",
       "1980-01-31  0.007752    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "1980-02-29 -0.086462    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "1980-03-31  0.000000    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "1980-04-30  0.139130    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "1980-05-31  0.036336    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "...              ...    ...  ...   ...       ...       ...       ...   \n",
       "2021-11-30       NaN    NaN  ...   NaN -0.001025 -0.033076  0.001111   \n",
       "2021-12-31       NaN    NaN  ...   NaN -0.001026  0.038516  0.054384   \n",
       "2022-01-31       NaN    NaN  ...   NaN -0.004620 -0.160989  0.084211   \n",
       "2022-02-28       NaN    NaN  ...   NaN  0.003610  0.065815  0.000000   \n",
       "2022-03-31       NaN    NaN  ...   NaN       NaN       NaN       NaN   \n",
       "\n",
       "                  ZY      ZYME      ZYNE      ZYXI  ZZ       ZZ1  \n",
       "datadate                                                          \n",
       "1980-01-31       NaN       NaN       NaN       NaN NaN  0.137500  \n",
       "1980-02-29       NaN       NaN       NaN       NaN NaN -0.164835  \n",
       "1980-03-31       NaN       NaN       NaN       NaN NaN -0.118421  \n",
       "1980-04-30       NaN       NaN       NaN       NaN NaN -0.029851  \n",
       "1980-05-31       NaN       NaN       NaN       NaN NaN  0.123077  \n",
       "...              ...       ...       ...       ...  ..       ...  \n",
       "2021-11-30 -0.145283 -0.096180 -0.096606  0.021548 NaN       NaN  \n",
       "2021-12-31 -0.261589 -0.184983 -0.167630 -0.221094 NaN       NaN  \n",
       "2022-01-31 -0.222720 -0.456986 -0.093750 -0.115145 NaN       NaN  \n",
       "2022-02-28 -0.263462 -0.186517 -0.134100 -0.205808 NaN       NaN  \n",
       "2022-03-31       NaN       NaN       NaN       NaN NaN       NaN  \n",
       "\n",
       "[507 rows x 32354 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns_df = pd.read_csv('returns.csv',index_col=0).loc[start_date:]\n",
    "# returns_df = returns_df.dropna(axis=0,thresh=500).dropna(axis=1,thresh=500)\n",
    "equity_idx = returns_df.columns\n",
    "date_idx = returns_df.index\n",
    "returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0c2309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>A2ME</th>\n",
       "      <th>AC</th>\n",
       "      <th>ATO</th>\n",
       "      <th>BEME</th>\n",
       "      <th>C</th>\n",
       "      <th>CF</th>\n",
       "      <th>CF2P</th>\n",
       "      <th>CTO</th>\n",
       "      <th>D2A</th>\n",
       "      <th>D2P</th>\n",
       "      <th>...</th>\n",
       "      <th>OP</th>\n",
       "      <th>PCM</th>\n",
       "      <th>PM</th>\n",
       "      <th>PROF</th>\n",
       "      <th>Q</th>\n",
       "      <th>RNA</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROE</th>\n",
       "      <th>S2P</th>\n",
       "      <th>SGA2S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tic</th>\n",
       "      <th>rdq</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0015B</th>\n",
       "      <th>1984-01-16</th>\n",
       "      <td>0.381941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.214994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-03-15</th>\n",
       "      <td>0.557109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.274170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-05-25</th>\n",
       "      <td>0.590922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.140065</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984-08-27</th>\n",
       "      <td>0.651735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.968086</td>\n",
       "      <td>0.247569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.418175</td>\n",
       "      <td>0.731681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985-01-01</th>\n",
       "      <td>0.871765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.760755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.778316</td>\n",
       "      <td>0.012021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.950109</td>\n",
       "      <td>0.230870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.175636</td>\n",
       "      <td>0.125346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.499765</td>\n",
       "      <td>0.748516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZZZ1</th>\n",
       "      <th>2000-08-14</th>\n",
       "      <td>1.171566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.523240</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098857</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-11-13</th>\n",
       "      <td>1.185838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.069810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.559099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.079706</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.104211</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-03-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.041668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.540736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075438</td>\n",
       "      <td>0.021779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-05-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029079</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.622717</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085226</td>\n",
       "      <td>0.044324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001-08-13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.653604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.091314</td>\n",
       "      <td>0.062878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924199 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      A2ME  AC       ATO  BEME         C  CF  CF2P       CTO  \\\n",
       "tic   rdq                                                                      \n",
       "0015B 1984-01-16  0.381941 NaN       NaN   NaN  0.214994 NaN   NaN       NaN   \n",
       "      1984-03-15  0.557109 NaN       NaN   NaN  0.274170 NaN   NaN       NaN   \n",
       "      1984-05-25  0.590922 NaN       NaN   NaN  0.140065 NaN   NaN       NaN   \n",
       "      1984-08-27  0.651735 NaN       NaN   NaN  0.009504 NaN   NaN       NaN   \n",
       "      1985-01-01  0.871765 NaN  0.760755   NaN  0.019574 NaN   NaN  0.778316   \n",
       "...                    ...  ..       ...   ...       ...  ..   ...       ...   \n",
       "ZZZ1  2000-08-14  1.171566 NaN  0.073420   NaN  0.036884 NaN   NaN  0.067789   \n",
       "      2000-11-13  1.185838 NaN  0.079706   NaN  0.029840 NaN   NaN  0.069810   \n",
       "      2001-03-28       NaN NaN  0.075438   NaN  0.041668 NaN   NaN  0.065717   \n",
       "      2001-05-14       NaN NaN  0.085226   NaN  0.029079 NaN   NaN  0.082268   \n",
       "      2001-08-13       NaN NaN  0.091314   NaN  0.040394 NaN   NaN  0.085981   \n",
       "\n",
       "                       D2A       D2P  ...  OP       PCM        PM  PROF   Q  \\\n",
       "tic   rdq                             ...                                     \n",
       "0015B 1984-01-16       NaN       NaN  ... NaN       NaN       NaN   NaN NaN   \n",
       "      1984-03-15       NaN       NaN  ... NaN       NaN       NaN   NaN NaN   \n",
       "      1984-05-25       NaN       NaN  ... NaN       NaN       NaN   NaN NaN   \n",
       "      1984-08-27  0.015213  0.000000  ... NaN -0.968086  0.247569   NaN NaN   \n",
       "      1985-01-01  0.012021  0.000000  ... NaN -0.950109  0.230870   NaN NaN   \n",
       "...                    ...       ...  ...  ..       ...       ...   ...  ..   \n",
       "ZZZ1  2000-08-14       NaN  0.006326  ... NaN  0.523240  1.000000   NaN NaN   \n",
       "      2000-11-13       NaN  0.004737  ... NaN  0.559099  1.000000   NaN NaN   \n",
       "      2001-03-28       NaN       NaN  ... NaN  0.540736  1.000000   NaN NaN   \n",
       "      2001-05-14       NaN       NaN  ... NaN  0.622717  1.000000   NaN NaN   \n",
       "      2001-08-13       NaN       NaN  ... NaN  0.653604  1.000000   NaN NaN   \n",
       "\n",
       "                       RNA       ROA  ROE       S2P     SGA2S  \n",
       "tic   rdq                                                      \n",
       "0015B 1984-01-16       NaN       NaN  NaN       NaN       NaN  \n",
       "      1984-03-15       NaN       NaN  NaN       NaN       NaN  \n",
       "      1984-05-25       NaN       NaN  NaN       NaN       NaN  \n",
       "      1984-08-27       NaN       NaN  NaN  1.418175  0.731681  \n",
       "      1985-01-01  0.175636  0.125346  NaN  1.499765  0.748516  \n",
       "...                    ...       ...  ...       ...       ...  \n",
       "ZZZ1  2000-08-14  0.073420  0.018000  NaN  0.098857       NaN  \n",
       "      2000-11-13  0.079706  0.021708  NaN  0.104211       NaN  \n",
       "      2001-03-28  0.075438  0.021779  NaN       NaN       NaN  \n",
       "      2001-05-14  0.085226  0.044324  NaN       NaN       NaN  \n",
       "      2001-08-13  0.091314  0.062878  NaN       NaN       NaN  \n",
       "\n",
       "[924199 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_comb_df = pd.read_csv('features_combined.csv').set_index(['tic','rdq']).drop(['FYEARQ','FQTR'],axis=1)\n",
    "feat_comb_df = feat_comb_df[sorted(feat_comb_df.columns)]\n",
    "feat_comb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2c50a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 507/507 [01:51<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "MIN_AVAILABLE_FEATURES = feat_comb_df.shape[1]//2\n",
    "\n",
    "full_df_lst = []\n",
    "for date in tqdm(date_idx):\n",
    "    sub = feat_comb_df.loc[feat_comb_df.index.get_level_values(1) < date]\n",
    "    sub = sub.groupby(sub.index.get_level_values('tic')).last(1)\n",
    "    sub = sub.transpose()\n",
    "    \n",
    "    sub = sub.rank(pct=True,axis=1)*2-1\n",
    "    idx = (sub.isna().sum(axis=0) >= MIN_AVAILABLE_FEATURES).index\n",
    "    sub = sub[idx].fillna(0)\n",
    "    \n",
    "#     sub = sub.fillna(sub.median(axis=1),axis=0)\n",
    "    \n",
    "    sub['date'] = date\n",
    "    sub =  sub.set_index(['date',sub.index])\n",
    "    \n",
    "    full_df_lst.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94bf341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58eb717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e6791e",
   "metadata": {},
   "source": [
    "# Train Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5ea7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = (48,360)\n",
    "valid = (360,420)\n",
    "test = (420,505)\n",
    "# Train: 1984 to 2010\n",
    "# Valid: 2010 to 2015\n",
    "# Test: 2015 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61b7aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, full_df_lst, returns_df, date_idx, start_i, end_i, rhs_input_assets=False,lag=1):\n",
    "        self.full_df_lst = full_df_lst\n",
    "        self.returns_df = returns_df\n",
    "        self.date_idx = date_idx\n",
    "        self.start_i, self.end_i = start_i, end_i\n",
    "        self.rhs_input_assets = rhs_input_assets\n",
    "        self.epsilon=1e-5\n",
    "        self.lag=lag\n",
    "        \n",
    "        self.output_date_idx = date_idx[start_i+lag:end_i]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.end_i - self.start_i - self.lag\n",
    "    def __getitem__(self, idx):\n",
    "        input_idx = idx\n",
    "        idx += self.start_i+self.lag\n",
    "        date = self.date_idx[idx] # returns date\n",
    "        \n",
    "        mask = ~np.isnan(self.returns_df.loc[date].to_numpy())\n",
    "        traded_assets_idx = self.returns_df.columns[mask]\n",
    "        \n",
    "        lhs = self.full_df_lst[idx - self.lag]\n",
    "        lhs_idx = lhs.columns\n",
    "        \n",
    "        comb_idx = traded_assets_idx.intersection(lhs_idx)\n",
    "        \n",
    "        lhs = lhs.reindex(comb_idx,axis=1,fill_value=0).to_numpy()\n",
    "        \n",
    "        rhs_tmp = self.returns_df.loc[date,comb_idx]\n",
    "        rhs_tmp = rhs_tmp.clip(rhs_tmp.quantile(0.01),rhs_tmp.quantile(0.99))\n",
    "        output = rhs_tmp.to_numpy()\n",
    "\n",
    "        lhs_tmp = np.concatenate([lhs,np.expand_dims(np.ones(lhs.shape[1]),0)],axis=0)\n",
    "        \n",
    "        \n",
    "        if self.rhs_input_assets:\n",
    "            rhs = output\n",
    "        else:\n",
    "            try:\n",
    "                rhs = (np.linalg.inv(lhs_tmp @ lhs_tmp.T) @ lhs_tmp) @ rhs_tmp\n",
    "            except Exception as e:\n",
    "                print(\"idx:\",input_idx)\n",
    "                print(e)\n",
    "                raise e\n",
    "        return lhs,rhs,output,comb_idx\n",
    "        \n",
    "        \n",
    "def collate_fn(batch):\n",
    "\n",
    "    len_x = [len(output) for lhs,rhs,output,_ in batch]\n",
    "    full_size = max(len_x)\n",
    "    \n",
    "    batch_lhs = [nn.functional.pad(torch.Tensor(lhs),pad=(0,full_size-lhs.shape[1],0,0)) for lhs,rhs,output,_ in batch]\n",
    "    batch_rhs = [torch.Tensor(rhs) for lhs,rhs,output,_ in batch]\n",
    "    batch_output = [nn.functional.pad(torch.Tensor(output),pad=(0,full_size-len(output))) for lhs,rhs,output,_ in batch]\n",
    "    batch_assets = [idx for lhs,rhs,output,idx in batch]\n",
    "    \n",
    "    return torch.cat([x.unsqueeze(0) for x in batch_lhs]),\\\n",
    "        torch.cat([x.unsqueeze(0) for x in batch_rhs]),\\\n",
    "        torch.cat([x.unsqueeze(0) for x in batch_output]), torch.tensor(len_x), batch_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf095288",
   "metadata": {},
   "source": [
    "# Conditional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d629c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalAutoEncoder(nn.Module):\n",
    "    def __init__(self, n_alphas, n_factors, n_hidden_lhs, n_hidden_rhs, \n",
    "                    batch_norm_lhs=True, batch_norm_rhs=False,\n",
    "                    dropout_p=0, initialization=True, rhs_input_assets = False, rhs_activation = True,\n",
    "                    bn_momentum=0.1, bn_track_running_stats = True):\n",
    "        # n_assets can be assets or portfolios:\n",
    "            # In both cases, we are interested in asset return or portfolio return on the RHS\n",
    "            # LHS is the original factor \n",
    "        super(ConditionalAutoEncoder, self).__init__()\n",
    "\n",
    "        self.n_alphas = n_alphas\n",
    "        self.n_factors = n_factors\n",
    "        self.n_hidden_lhs = n_hidden_lhs\n",
    "        self.n_hidden_rhs = n_hidden_rhs\n",
    "\n",
    "        self.lhs_lst = []\n",
    "\n",
    "        prev_lhs_layer_size = self.n_alphas\n",
    "        layer_lst = []\n",
    "        for v in n_hidden_lhs+[n_factors]:\n",
    "            if batch_norm_lhs:\n",
    "                # layer_lst.append(nn.BatchNorm1d(prev_lhs_layer_size,momentum=bn_momentum,\n",
    "                #         track_running_stats=bn_track_running_stats))\n",
    "                layer_lst.append(nn.LayerNorm(prev_lhs_layer_size))\n",
    "            if dropout_p:\n",
    "                layer_lst.append(nn.Dropout(dropout_p))\n",
    "            layer_lst.append(nn.Linear(prev_lhs_layer_size, v, bias=True))\n",
    "            if initialization:\n",
    "                nn.init.kaiming_normal_(layer_lst[-1].weight,mode='fan_in',nonlinearity='linear')\n",
    "            layer_lst.append(nn.SELU())\n",
    "            prev_lhs_layer_size = v\n",
    "\n",
    "        # if batch_norm_lhs:\n",
    "        #     layer_lst.append(nn.BatchNorm1d(prev_lhs_layer_size,momentum=bn_momentum,\n",
    "        #             track_running_stats=bn_track_running_stats))\n",
    "        #     # layer_lst.append(nn.LayerNorm(prev_lhs_layer_size))\n",
    "        # if dropout_p:\n",
    "        #     layer_lst.append(nn.Dropout(dropout_p))\n",
    "        # layer_lst.append(nn.Linear(prev_lhs_layer_size, self.n_factors, bias=True))\n",
    "        # if initialization:\n",
    "        #     nn.init.xavier_normal_(layer_lst[-1].weight)\n",
    "        self.lhs = nn.Sequential(*layer_lst)\n",
    "\n",
    "        # self.lhs_lst = nn.ModuleList(self.lhs_lst)\n",
    "\n",
    "        rhs_lst = []\n",
    "#         if rhs_input_assets:\n",
    "#             if batch_norm_rhs:\n",
    "#                 # rhs_lst.append(nn.BatchNorm1d(self.n_assets,momentum=bn_momentum,\n",
    "#                 #     track_running_stats=bn_track_running_stats))\n",
    "#                 rhs_lst.append(nn.LayerNorm(self.n_assets))\n",
    "#             if dropout_p:\n",
    "#                 rhs_lst.append(nn.Dropout(dropout_p))\n",
    "#             rhs_lst.append(nn.Linear(n_assets,n_alphas+1,bias=True))\n",
    "#             if initialization:\n",
    "#                 nn.init.kaiming_normal_(rhs_lst[-1].weight,mode='fan_in',nonlinearity='linear')\n",
    "#             if rhs_activation:\n",
    "#                 rhs_lst.append(nn.SELU())\n",
    "\n",
    "        prev_rhs_layer_size = self.n_alphas+1\n",
    "        for v in n_hidden_rhs+[n_factors]:\n",
    "            if batch_norm_rhs:\n",
    "              # rhs_lst.append(nn.BatchNorm1d(prev_rhs_layer_size,momentum=bn_momentum,\n",
    "              #         track_running_stats=bn_track_running_stats))\n",
    "                rhs_lst.append(nn.LayerNorm(prev_rhs_layer_size))\n",
    "            if dropout_p:\n",
    "                rhs_lst.append(nn.Dropout(dropout_p))\n",
    "            rhs_lst.append(nn.Linear(prev_rhs_layer_size,v,bias=True))\n",
    "            if initialization:\n",
    "                nn.init.kaiming_normal_(rhs_lst[-1].weight,mode='fan_in',nonlinearity='linear')\n",
    "            if rhs_activation:\n",
    "                rhs_lst.append(nn.SELU())\n",
    "            prev_rhs_layer_size = v\n",
    "        \n",
    "        self.rhs = nn.Sequential(*rhs_lst)\n",
    "\n",
    "#         self.final = nn.Linear(n_assets,n_assets,bias=True)\n",
    "\n",
    "    def forward(self, x_lhs, x_rhs):\n",
    "        # idx = 0\n",
    "        y_lhs_lst = []\n",
    "        for i in range(x_lhs.shape[2]):\n",
    "            # y_cur = self.lhs_lst[i](x_lhs[:,idx:idx+self.n_alphas])\n",
    "            y_cur = self.lhs(x_lhs[:,:,i])\n",
    "            y_cur = torch.unsqueeze(y_cur,2)\n",
    "            y_lhs_lst.append(y_cur)\n",
    "            # idx += self.n_alphas\n",
    "        y_lhs = torch.cat(y_lhs_lst,dim=2)\n",
    "\n",
    "        y_rhs = self.rhs(x_rhs).unsqueeze(dim=2)\n",
    "        self.last_rhs_factors = y_rhs\n",
    "        out = torch.bmm(y_lhs.transpose(1,2), y_rhs).squeeze(2)\n",
    "#         out = self.final(out)\n",
    "        return out, y_rhs\n",
    "\n",
    "    def forward_given_factors(self, x_lhs, factors):\n",
    "        y_lhs_lst = []\n",
    "        for i in range(x_lhs.shape[2]):\n",
    "            # y_cur = self.lhs_lst[i](x_lhs[:,idx:idx+self.n_alphas])\n",
    "            y_cur = self.lhs(x_lhs[:,:,i])\n",
    "            y_cur = torch.unsqueeze(y_cur,2)\n",
    "            y_lhs_lst.append(y_cur)\n",
    "            # idx += self.n_alphas\n",
    "        y_lhs = torch.cat(y_lhs_lst,dim=2)\n",
    "\n",
    "        y_rhs = factors\n",
    "        out = torch.bmm(y_lhs.transpose(1,2), y_rhs).squeeze(2)\n",
    "#         out = self.final(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21848353",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15bfca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "@print_durations()\n",
    "def test(args,model,device,test_loader,criterion):\n",
    "    model.eval()\n",
    "        \n",
    "    output_lst = []\n",
    "    data_lst = []\n",
    "\n",
    "    for data_lhs, data_rhs, data_output, data_x_lens, data_assets in test_loader:\n",
    "        # print(\"Shapes:\",data_lhs.shape, data_rhs.shape, data_output.shape, data_mask.shape)\n",
    "        data_lhs = data_lhs.float().to(device)\n",
    "        data_rhs = data_rhs.float().to(device)\n",
    "        data_output = data_output.float().to(device)\n",
    "        data_x_lens = data_x_lens.int().to(device)\n",
    "        output,_ = model(x_lhs = data_lhs, x_rhs = data_rhs)\n",
    "\n",
    "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
    "        output_masked = torch.masked_select(output,data_mask)\n",
    "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
    "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
    "        # print(\"Loss Inside:\",criterion(output_masked,data_output_masked))\n",
    "\n",
    "        output_lst.append(output_masked)\n",
    "        data_lst.append(data_output_masked)\n",
    "    \n",
    "    full_output = torch.cat(output_lst,axis=0)\n",
    "    full_data = torch.cat(data_lst,axis=0)\n",
    "\n",
    "    return mean_squared_error(full_data.cpu().detach().numpy(), full_output.cpu().detach().numpy())\n",
    "\n",
    "@print_durations()\n",
    "def train(args, model, device, train_loader, optimizer, criterion, epoch, max_norm=None):\n",
    "    model.train()\n",
    "    \n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
    "    \n",
    "    total_loss = 0\n",
    "    total_reg_loss=0\n",
    "    \n",
    "    N = len(train_loader)\n",
    "    \n",
    "    for batch_idx, (data_lhs, data_rhs, data_output, data_x_lens, data_assets) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data_lhs = data_lhs.float().to(device)\n",
    "        data_rhs = data_rhs.float().to(device)\n",
    "        data_output = data_output.float().to(device)\n",
    "        data_x_lens = data_x_lens.int().to(device)\n",
    "\n",
    "        if max_norm: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        output,_ = model(x_lhs = data_lhs, x_rhs = data_rhs)\n",
    "        \n",
    "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
    "\n",
    "        output_masked = torch.masked_select(output,data_mask)\n",
    "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
    "\n",
    "        loss_tmp = criterion(output_masked, data_output_masked)\n",
    "\n",
    "        regularization_loss = 0\n",
    "        for p in model.parameters():\n",
    "            regularization_loss += torch.sum(abs(p))\n",
    "\n",
    "        loss = loss_tmp + args['l1_lambda'] * regularization_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "        total_reg_loss += regularization_loss * args['l1_lambda'] \n",
    "\n",
    "        # print(\"Gradient:\", loss.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            total_loss=\"{:.04f}\".format(float(total_loss / (batch_idx + 1))),\n",
    "            reg_loss = \"{:.04f}\".format(float(total_reg_loss / (batch_idx + 1))),\n",
    "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "        \n",
    "    print(f\"Epoch {epoch}, total_loss={float(total_loss / N):.04f}, reg_loss = {float(total_reg_loss / N):.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0fdbceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = AEDataSet(full_df_lst,returns_df,date_idx,48,360)\n",
    "# valid_data = AEDataSet(full_df_lst,returns_df,date_idx,360,420)\n",
    "# test_data= AEDataSet(full_df_lst,returns_df,date_idx,420,505)\n",
    "\n",
    "def run(args, n_alphas, full_df_lst, returns_df, date_idx, train_start,\n",
    "        train_end, valid_start, valid_end, n_window=36, batch_size=16):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ConditionalAutoEncoder(\n",
    "        n_alphas = n_alphas,\n",
    "        n_factors = args['n_factors'],\n",
    "        n_hidden_lhs = args['n_hidden_lhs'],\n",
    "        n_hidden_rhs = args['n_hidden_rhs'],\n",
    "        batch_norm_lhs = args['batch_norm_lhs'],\n",
    "        batch_norm_rhs = args['batch_norm_rhs'],\n",
    "        dropout_p=args['dropout_p'],\n",
    "        initialization=args['initialization'],\n",
    "        rhs_input_assets=args['rhs_input_assets'],\n",
    "        rhs_activation = args['rhs_activation']\n",
    "    #     bn_track_running_stats=args['bn_track_running_stats'],\n",
    "    #     bn_momentum=args['bn_momentum']\n",
    "    )\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_data = AEDataSet(full_df_lst,returns_df,date_idx,train_start,train_end)\n",
    "    valid_data = AEDataSet(full_df_lst,returns_df,date_idx,valid_start,valid_end)\n",
    "    extended_valid_data = AEDataSet(full_df_lst,returns_df,date_idx,valid_start-n_window,valid_end)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size = batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        shuffle=True,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_data,\n",
    "        batch_size = batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "    \n",
    "    extended_valid_loader = torch.utils.data.DataLoader(\n",
    "        extended_valid_data,\n",
    "        batch_size = batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args['lr'], weight_decay=args['l2_lambda'])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_mse = test(args,model,device,train_loader,criterion)\n",
    "    valid_mse = test(args,model,device,valid_loader,criterion)\n",
    "    train_mse_lst, valid_mse_lst = [train_mse], [valid_mse]\n",
    "\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=args['gamma'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,'min',0.5,patience=5,threshold=1e-3,cooldown=10)\n",
    "    print(\"Pre-Train Train MSE:\", train_mse)\n",
    "    print(\"Pre-Train Valid MSE:\", valid_mse)\n",
    "\n",
    "    best_model = None\n",
    "    for epoch in range(1, args['epoch']+1):\n",
    "        log_dict = {}\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        train(args,model,device,train_loader,optimizer,criterion,epoch,max_norm=args['max_norm'])\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        log_dict['train_mse'] = test(args,model,device,train_loader,criterion)\n",
    "        log_dict['valid_mse'] = test(args,model,device,valid_loader,criterion)\n",
    "        log_dict['lr'] = scheduler.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Train MSE:\",log_dict['train_mse'])\n",
    "        print(\"Valid MSE:\",log_dict['valid_mse'])\n",
    "\n",
    "        scheduler.step(log_dict['valid_mse'])\n",
    "        print('Next Learning Rate:', scheduler.optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "        print(train_mse_lst)\n",
    "        print(valid_mse_lst)\n",
    "        if log_dict['valid_mse'] < np.min(valid_mse_lst):\n",
    "            best_model = model\n",
    "            torch.save(best_model, './best_model.model')\n",
    "            # torch.save(model.state_dict(), args['MODEL_PATH']+\"/model_epoch_\"+str(epoch))\n",
    "\n",
    "        train_mse_lst.append(log_dict['train_mse'])\n",
    "        valid_mse_lst.append(log_dict['valid_mse'])\n",
    "\n",
    "        if len(valid_mse_lst)>args['patience'] and log_dict['valid_mse'] > \\\n",
    "            np.min(valid_mse_lst[:-args['patience']]):\n",
    "                break\n",
    "    \n",
    "    print(\"Training done. Evaluating...\")\n",
    "    \n",
    "    output_lst = []\n",
    "    data_lst = []\n",
    "    factors_lst = []\n",
    "\n",
    "    for data_lhs, data_rhs, data_output, data_x_lens, data_assets in tqdm(extended_test_loader):\n",
    "        # print(\"Shapes:\",data_lhs.shape, data_rhs.shape, data_output.shape, data_mask.shape)\n",
    "        data_lhs = data_lhs.float().to(device)\n",
    "        data_rhs = data_rhs.float().to(device)\n",
    "        data_output = data_output.float().to(device)\n",
    "        data_x_lens = data_x_lens.int().to(device)\n",
    "        output, factors = best_model(x_lhs = data_lhs, x_rhs = data_rhs)\n",
    "\n",
    "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
    "        output_masked = torch.masked_select(output,data_mask)\n",
    "        data_output_masked = torch.masked_select(data_output,data_mask)\n",
    "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
    "\n",
    "        indices = [0]+list(torch.cumsum(data_x_lens,axis=0).cpu().numpy())\n",
    "\n",
    "        for k,(i,j) in enumerate(zip(indices[:-1],indices[1:])):\n",
    "            output_lst.append(pd.Series(output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
    "            data_lst.append(pd.Series(data_output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
    "            factors_lst.append(pd.Series(factors[k].squeeze().cpu().detach().numpy()))\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    output_df = pd.DataFrame(output_lst,index=extended_valid_data.output_date_idx).iloc[n_window:]\n",
    "    data_output_df = pd.DataFrame(data_lst,index=extended_valid_data.output_date_idx).iloc[n_window:]\n",
    "    factors_df = pd.DataFrame(factors_lst,index=extended_valid_data.output_date_idx)\n",
    "    \n",
    "    predicted_factors = factors_df.rolling(n_window).mean()\n",
    "    factors_tensor = torch.tensor(predicted_factors.iloc[n_window:].to_numpy()).unsqueeze(2).float().to(device)\n",
    "    \n",
    "    \n",
    "    pred_output_lst = []\n",
    "    for i, (data_lhs, _, _, data_x_lens, data_assets) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
    "        data_lhs = data_lhs.float().to(device)\n",
    "        data_x_lens = data_x_lens.int().to(device)\n",
    "        output= best_model.forward_given_factors(x_lhs = data_lhs, factors = factors_tensor[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        data_mask = torch.arange(data_lhs.shape[2])[None, :].to(device) < data_x_lens[:, None]\n",
    "        output_masked = torch.masked_select(output,data_mask)\n",
    "        assert (len(output_masked) == torch.sum(data_x_lens))\n",
    "\n",
    "        indices = [0]+list(torch.cumsum(data_x_lens,axis=0).cpu().numpy())\n",
    "\n",
    "        for k,(i,j) in enumerate(zip(indices[:-1],indices[1:])):\n",
    "            pred_output_lst.append(pd.Series(output_masked[i:j].cpu().detach().numpy(),index=data_assets[k]))\n",
    "    tm1_predicted_returns = pd.DataFrame(output_lst,index=predicted_factors.iloc[n_window:].index)\n",
    "    predicted_returns = tm1_predicted_returns.shift(1).iloc[1:]\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_dict = {}\n",
    "    \n",
    "    # Predictive Stats:\n",
    "    # Spearman Corr\n",
    "    corr = predicted_returns.corrwith(returns_df.loc[predicted_returns.index,predicted_returns.columns],axis=1,method='spearman')\n",
    "    log_dict['pred_spearman_corr'] = corr.mean()\n",
    "    # R2\n",
    "    numer = (returns_df.loc[predicted_returns.index,predicted_returns.columns]-predicted_returns)\n",
    "    numer_mask = (~numer.isna()).astype(int)\n",
    "    denom = returns_df.loc[predicted_returns.index,predicted_returns.columns] * numer_mask\n",
    "    log_dict['pred_r2'] = 1 - ((numer**2).sum(axis=1))/(denom**2).sum(axis=1)\n",
    "    \n",
    "    # Attribution Stats:\n",
    "    # Spearman Corr\n",
    "    corr = output_df.corrwith(returns_df.loc[output_df.index,output_df.columns],axis=1,method='spearman')\n",
    "    log_dict['total_spearman_corr'] = corr.mean()\n",
    "    # R2\n",
    "    numer = (returns_df.loc[output_df.index,output_df.columns]-output_df)\n",
    "    numer_mask = (~numer.isna()).astype(int)\n",
    "    denom = returns_df.loc[output_df.index,output_df.columns] * numer_mask\n",
    "    log_dict['total_r2'] = 1 - ((numer**2).sum(axis=1))/(denom**2).sum(axis=1)\n",
    "    \n",
    "    # Trading Stats:\n",
    "    train_pred_rank = predicted_returns.rank(pct=True,axis=1,numeric_only=True,ascending=True)\n",
    "    long_port = ((train_pred_rank >= 0.9) & (train_pred_rank < 1)).astype(int)\n",
    "    long_port = long_port.divide(long_port.sum(axis=1),axis=0)\n",
    "    long_port_returns = (long_port*returns_df.loc[long_port.index]).sum(axis=1)\n",
    "    short_port = (train_pred_rank <= 0.1).astype(int)\n",
    "    short_port = short_port.divide(short_port.sum(axis=1),axis=0)\n",
    "    short_port_returns = (short_port*returns_df.loc[short_port.index]).sum(axis=1)\n",
    "    comb_port = long_port - short_port\n",
    "    comb_port_returns = (comb_port*returns_df.loc[comb_port.index]).sum(axis=1)\n",
    "    log_dict['long_short_SR'] = (comb_port_returns.mean()/comb_port_returns.std())*np.sqrt(12)\n",
    "    log_dict['long_short_returns'] = comb_port_returns.mean()*12\n",
    "    log_dict['long_short_SD'] = comb_port_returns.std()*np.sqrt(12)\n",
    "    \n",
    "    log_dict['long_SR'] = (long_port_returns.mean()/long_port_returns.std())*np.sqrt(12)\n",
    "    log_dict['long_returns'] = long_port_returns.mean()*12\n",
    "    log_dict['long_SD'] = long_port_returns.std()*np.sqrt(12)\n",
    "    \n",
    "    log_dict['short_SR'] = (short_port_returns.mean()/short_port_returns.std())*np.sqrt(12)\n",
    "    log_dict['short_returns'] = short_port_returns.mean()*12\n",
    "    log_dict['short_SD'] = short_port_returns.std()*np.sqrt(12)\n",
    "    \n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222ea8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_args = {\n",
    "#     'n_factors' : 5,\n",
    "#     'n_hidden_lhs':[16,8],\n",
    "    'n_hidden_rhs':[],\n",
    "    'batch_norm_lhs' : True,\n",
    "    'batch_norm_rhs' : False,\n",
    "    'rhs_activation': True,\n",
    "    'dropout_p' : 0.0,\n",
    "    'max_norm' : 1,\n",
    "    'bn_momentum': 0.01,\n",
    "    'bn_track_running_stats': True,\n",
    "    'initialization' : False,\n",
    "#     'rhs_input_assets' : False,\n",
    "    'lr' : 5e-4,\n",
    "    'gamma': 1/4,\n",
    "    'l2_lambda': 0,\n",
    "    'l1_lambda': 4e-3,\n",
    "    'batch_size': 32,\n",
    "    'log_interval': 1,\n",
    "    'epoch': 40,\n",
    "    'patience': 6\n",
    "}\n",
    "\n",
    "search_args = {\n",
    "    'n_factors': [2,4,6],\n",
    "    'n_hidden_lhs': [[24],[16,8],[16,8,4]],\n",
    "    'rhs_input_assets': [True,False]\n",
    "}\n",
    "\n",
    "search_keys = list(search_args.keys())\n",
    "search_values = list(search_args.values())\n",
    "from itertools import product\n",
    "search_values = list(product(*search_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "916fbf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, [24], True),\n",
       " (2, [24], False),\n",
       " (2, [16, 8], True),\n",
       " (2, [16, 8], False),\n",
       " (2, [16, 8, 4], True),\n",
       " (2, [16, 8, 4], False),\n",
       " (4, [24], True),\n",
       " (4, [24], False),\n",
       " (4, [16, 8], True),\n",
       " (4, [16, 8], False),\n",
       " (4, [16, 8, 4], True),\n",
       " (4, [16, 8, 4], False),\n",
       " (6, [24], True),\n",
       " (6, [24], False),\n",
       " (6, [16, 8], True),\n",
       " (6, [16, 8], False),\n",
       " (6, [16, 8, 4], True),\n",
       " (6, [16, 8, 4], False)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0447acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params 0: (2, [24], True)\n"
     ]
    }
   ],
   "source": [
    "res_dict = {}\n",
    "for i,tpl in enumerate(search_values):\n",
    "    args = fixed_args.copy()\n",
    "    for k,v in zip(search_keys,tpl):\n",
    "        args[k] = v\n",
    "    print(f\"Params {i}:\", tpl)\n",
    "    log_dict = run(args,n_alphas=29, full_df_lst=full_df_lst, returns_df=returns_df, date_idx=date_idx, \n",
    "                   train_start=48, train_end=360, valid_start=360, valid_end=420, \n",
    "                   n_window=36, batch_size=16)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    res_dict[tpl] = log_dict\n",
    "    \n",
    "    with open('res_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(res_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    !cp * ../efs/project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59fd564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
