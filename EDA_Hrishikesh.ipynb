{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7288461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "import gc\n",
    "from scipy.stats import pearsonr as corr\n",
    "from os.path import exists\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c457288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = 'trt1m' #use log instead\n",
    "eps = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c640c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRSP  = pd.read_csv('CRSP.csv', usecols=['pstkq', 'txditcq', 'fyearq','fqtr','conm', 'atq', 'cshoq', 'ibq', 'prchq', 'tic','rdq', 'teqq', 'datadate'], parse_dates=['rdq', 'datadate'])\n",
    "Sec_m = pd.read_csv('Sec.csv' , usecols=['prccm', 'prchm', 'trt1m', 'trfm', 'cshtrm', 'tic','datadate'], parse_dates=['datadate'])\n",
    "ff_m  = pd.read_csv('F-F.csv', skiprows=3)[:-98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83de7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CRSP                = CRSP.sort_values('datadate') #drop index na instead\n",
    "CRSP = CRSP.set_index(['tic','datadate'])\n",
    "CRSP = CRSP.sort_index()\n",
    "CRSP = CRSP.loc[CRSP.index.dropna()]\n",
    "CRSP = CRSP.drop_duplicates()\n",
    "CRSP                = CRSP.reset_index().sort_values('datadate')\n",
    "for i in CRSP.columns:\n",
    "    if i not in ['tic', 'datadate', 'rdq', 'conm']:\n",
    "        CRSP[i]      = pd.to_numeric(CRSP[i])\n",
    "\n",
    "Sec_m               = Sec_m.sort_values('datadate')\n",
    "for i in Sec_m.columns:\n",
    "    if i not in ['tic', 'datadate']:\n",
    "        Sec_m[i]      = pd.to_numeric(Sec_m[i])\n",
    "\n",
    "ff_m                = ff_m.dropna()\n",
    "ff_m['date']        = pd.to_datetime(ff_m[ff_m.columns[0]], format='%Y%m')\n",
    "ff_m                = ff_m.drop(columns=ff_m.columns[0], axis=1)\n",
    "for i in ff_m.columns:\n",
    "    if i not in ['date']:\n",
    "        ff_m[i]      = pd.to_numeric(ff_m[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f82c686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge_asof(Sec_m, CRSP, on='datadate', by='tic')\n",
    "df = pd.merge_asof(df, ff_m, right_on='date', left_on='datadate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6180c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['tic', 'datadate']).set_index(['tic','datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e9d1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_d                = pd.read_csv('F-F_d.csv', skiprows=3)[:-1]\n",
    "ff_d                = ff_d.dropna()\n",
    "ff_d['date']        = pd.to_datetime(ff_d[ff_d.columns[0]], format='%Y%m%d')\n",
    "ff_d['Mkt-RF']      = pd.to_numeric(ff_d['Mkt-RF'])\n",
    "ff_d['SMB']         = pd.to_numeric(ff_d['SMB'])\n",
    "ff_d['HML']         = pd.to_numeric(ff_d['HML'])\n",
    "ff_d['RF']          = pd.to_numeric(ff_d['RF'])\n",
    "\n",
    "#df_d = pd.merge_asof(Sec_d, ff_d, right_on='date', left_on='datadate')\n",
    "#df_d = df_d.dropna().sort_values(['tic', 'datadate']).set_index(['tic','datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a029b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff347c90d0a2431aa6aa6475878deea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32354 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def single_sub(sub): # .loc[['AAPL','GME']]\n",
    "    try:\n",
    "        sub = sub.copy()\n",
    "        sub = sub.reset_index().set_index('datadate')\n",
    "        tic = np.unique(sub['tic'])[0]\n",
    "\n",
    "        file = f'data/{tic}_daily.csv'\n",
    "        e = exists(file)\n",
    "\n",
    "        if e:\n",
    "            Sec_d = pd.read_csv(file, usecols=['prccd', 'cshtrd', 'trfd', 'tic','datadate'], parse_dates=['datadate'])\n",
    "            Sec_d = Sec_d.set_index(['datadate'])\n",
    "            Sec_d = Sec_d.sort_index()\n",
    "            Sec_d = Sec_d.loc[Sec_d.index.dropna()]\n",
    "            Sec_d = Sec_d.drop_duplicates()\n",
    "            Sec_d = Sec_d.reset_index()\n",
    "            df_d = pd.merge_asof(Sec_d[Sec_d['tic'] == tic], ff_d, right_on='date', left_on='datadate')\n",
    "            df_d = df_d.set_index(['datadate'])\n",
    "            df_d['returns'] = df_d['prccd'] * df_d['trfd']\n",
    "            df_d['returns'] = df_d['returns']/(df_d['returns'].shift(1)+eps)\n",
    "            df_d['returns'] = (df_d['returns'] - 1) * 100\n",
    "\n",
    "        sub['r2_1'] = sub[returns].shift(2)\n",
    "        sub['r36_13'] = 0.0\n",
    "        for i in range(13, 37):\n",
    "            sub['r36_13'] += sub[returns].shift(i)\n",
    "        sub['r12_7'] = 0.0\n",
    "        for i in range(7, 13):\n",
    "            sub['r12_7'] += sub[returns].shift(i)\n",
    "        sub['r12_2'] = sub['r12_7']\n",
    "        for i in range(2, 7):\n",
    "            sub['r12_2'] += sub[returns].shift(i)\n",
    "        sub['ST_Rev'] = sub[returns].shift(1)\n",
    "        sub['LT_Rev'] = sub['r36_13']\n",
    "        for i in range(37, 61):\n",
    "            sub['LT_Rev'] += sub[returns].shift(i)\n",
    "        sub['AT'] = sub['atq']\n",
    "        sub['LME'] = sub['prccm'] * sub['cshoq']\n",
    "        sub['LTurnover'] = sub['cshtrm'] / (sub['cshoq']+eps)\n",
    "\n",
    "        sub['Beta'] = np.nan\n",
    "        sub['Resid_Var'] = np.nan\n",
    "        sub['IdioVol'] = np.nan\n",
    "        sub['SUV'] = np.nan\n",
    "        sub['Variance'] = np.nan\n",
    "        if e:\n",
    "            for i in range(61, len(sub)):\n",
    "                #TODO: They use Log excess returns - which don't make sense - since excess returns can be negative\n",
    "                daily = df_d.loc[sub.index[max(0, i-60)]:sub.index[i]].reset_index()[['Mkt-RF', 'returns', 'RF', 'date']].dropna()\n",
    "                if len(daily) < 750:\n",
    "                    continue\n",
    "                excess_ret = daily['returns'] - daily['RF']\n",
    "                #log_excess_ret = np.log(excess_ret)\n",
    "                excess_ret_3 = excess_ret + excess_ret.shift(1) + excess_ret.shift(2)\n",
    "                #log_excess_ret_3 = np.log(excess_ret_3)\n",
    "                #mkt_excess_ret = daily['Mkt-RF']\n",
    "                mkt_excess_ret = daily['Mkt-RF']\n",
    "                mkt_excess_ret_3 = mkt_excess_ret + mkt_excess_ret.shift(1) + mkt_excess_ret.shift(2)\n",
    "                #mkt_log_excess_ret_3 = np.log(mkt_excess_ret_3)\n",
    "                try:\n",
    "                    sub.loc[sub.index[i], 'Beta'] = corr(excess_ret_3.iloc[2:], mkt_excess_ret_3.iloc[2:])[0] * np.std(excess_ret.iloc[2:][daily.iloc[2:]['date'] >= sub.index[i-12]]) / (np.std(excess_ret.iloc[2:][daily.iloc[2:]['date'] >= sub.index[i-12]])+eps)\n",
    "                except Exception as e:\n",
    "                    print(daily, excess_ret_3.iloc[2:], mkt_excess_ret_3.iloc[2:])\n",
    "                    logging.exception(e)\n",
    "                    raise\n",
    "\n",
    "            for i in range(12, len(sub)):\n",
    "                daily = df_d.loc[sub.index[max(0, i-12)]:sub.index[i]].reset_index()[['Mkt-RF', 'SMB', 'HML', 'returns', 'RF']].dropna()\n",
    "                if len(daily) < 120:\n",
    "                    continue\n",
    "                reg =  LR().fit(daily[['Mkt-RF', 'SMB', 'HML']], (daily['returns'] - daily['RF']))\n",
    "                pred = reg.predict(daily[['Mkt-RF', 'SMB', 'HML']])\n",
    "                residual = (daily['returns'] - daily['RF'] - pred)\n",
    "                sub.loc[sub.index[i], 'IdioVol'] = np.std(residual)\n",
    "\n",
    "\n",
    "            for i in range(2, len(sub)):\n",
    "                daily = df_d.loc[sub.index[max(0, i-2)]:sub.index[i]].reset_index()[['Mkt-RF', 'SMB', 'HML', 'returns', 'RF']].dropna()\n",
    "                if len(daily) < 22:\n",
    "                    continue\n",
    "                reg =  LR().fit(daily[['Mkt-RF', 'SMB', 'HML']], (daily['returns'] - daily['RF']))\n",
    "                pred = reg.predict(daily[['Mkt-RF', 'SMB', 'HML']])\n",
    "                residual = (daily['returns'] - daily['RF'] - pred)\n",
    "                sub.loc[sub.index[i], 'Resid_Var'] = np.std(residual)\n",
    "\n",
    "            for i in range(13, len(sub)):\n",
    "                daily = df_d.loc[sub.index[max(0, i-13)]:sub.index[i-1]].reset_index()[['cshtrd', 'returns', 'RF']].dropna()\n",
    "                if len(daily) < 120:\n",
    "                    continue\n",
    "                daily2 = df_d.loc[sub.index[max(0, i-1)]:sub.index[i]].reset_index().dropna()\n",
    "                if len(daily2) < 10:\n",
    "                    continue\n",
    "                reg =  LR().fit(np.abs(daily['returns'] - daily['RF']).values.reshape(-1, 1), daily['cshtrd'])\n",
    "                pred = reg.predict((daily2['returns'] - daily2['RF']).values.reshape(-1, 1))\n",
    "                residual = (daily2['cshtrd'] - pred)\n",
    "                sub.loc[sub.index[i], 'SUV'] = np.mean(residual) / (np.std(residual)+eps)\n",
    "\n",
    "            for i in range(2, len(sub)):\n",
    "                daily = df_d.loc[sub.index[max(0, i-2)]:sub.index[i]].reset_index()\n",
    "                if len(daily) < 22:\n",
    "                    continue\n",
    "                sub.loc[sub.index[i], 'Variance'] = np.var(daily['returns'])\n",
    "            #TODO: Use monthly data where daily is unavailable\n",
    "\n",
    "        sub['MktBeta'] = np.nan\n",
    "        for i in range(61, len(sub)):\n",
    "            slice_ = sub.iloc[i-61:i][['Mkt-RF', 'SMB', 'HML', 'RF', returns]].dropna()\n",
    "            if len(slice_) < 30:\n",
    "                    continue\n",
    "            sub.loc[sub.index[i], 'MktBeta'] = LR().fit(slice_[['Mkt-RF', 'SMB', 'HML']], (slice_[returns] - slice_['RF'])).coef_[0]\n",
    "\n",
    "        sub['Rel2High'] = sub['prccm']/(sub['prchm'].rolling(12).max()+eps)\n",
    "        sub['ROE'] = sub['ibq'].rolling(4).sum()/((sub['teqq']+sub['txditcq']-sub['pstkq']).shift(4)+eps)\n",
    "\n",
    "        sub = sub.reset_index()\n",
    "        #df2.append(sub[['datadate', 'tic', 'fyearq', 'fqtr', 'conm', 'datadate', 'rdq', 'r2_1', 'r36_13', 'r12_7', 'r12_2', 'ST_Rev', 'LT_Rev', 'AT', 'LME', 'LTurnover', 'Beta', 'MktBeta', 'IdioVol', 'Resid_Var', 'SUV', 'Rel2High', 'ROE', 'Variance']])\n",
    "        return sub[['datadate', 'tic', 'fyearq', 'fqtr', 'conm', 'datadate', 'rdq', 'r2_1', 'r36_13', 'r12_7', 'r12_2', 'ST_Rev', 'LT_Rev', 'AT', 'LME', 'LTurnover', 'Beta', 'MktBeta', 'IdioVol', 'Resid_Var', 'SUV', 'Rel2High', 'ROE', 'Variance']]\n",
    "    except Exception as e:\n",
    "        logging.exception(e)\n",
    "        raise\n",
    "\n",
    "#df2 = list(map(single_sub, [i for _, i in df.groupby('tic')]))\n",
    "df2 = process_map(single_sub, [i for _, i in df.groupby('tic')], max_workers=os.cpu_count()-2, chunksize=10)\n",
    "#df2 = pd.concat(df2,axis=0)\n",
    "#df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730c8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat(df2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e908bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML1_pdp]",
   "language": "python",
   "name": "conda-env-ML1_pdp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
